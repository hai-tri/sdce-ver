# TinyLlama 1.1B on SlimPajama - TPU v4-32 Configuration
# ========================================================

model:
  name_or_path: "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
  dtype: "bfloat16"
  use_flash_attention: false
  gradient_checkpointing: true
  trust_remote_code: false
  init_from_scratch: false

surrogate:
  name_or_path: "Qwen/Qwen2.5-1.5B"
  dtype: "bfloat16"
  k: 30
  probability_threshold: 0.02
  enabled: true
  trust_remote_code: true
  loss_weight_initial: 1.0
  loss_weight_final: 0.0
  use_perplexity_weighting: true

data:
  dataset_name: "cerebras/SlimPajama-627B"
  dataset_config: null
  dataset_split: "train"
  eval_split: "validation"
  text_column: "text"
  max_seq_length: 2048
  preprocessing_num_workers: 2
  eval_split_ratio: 0.001
  eval_split_seed: 42

training:
  output_dir: "gs://sdce-tinyllama-checkpoints/tinyllama-slimpajama"
  num_epochs: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_ratio: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  loss_type: "surrogate"
  device: "tpu"
  tpu_cores: 8
  tpu_num_hosts: 4
  tpu_metrics_debug: false
  tpu_use_pjrt: true
  mixed_precision: "bf16"
  logging_steps: 100
  eval_steps: 5000
  save_steps: 5000
  save_total_limit: 5
  use_z_loss: true
  z_loss_multiplier: 1.0e-4
  seed: 42
  wandb_project: "SDCE-TinyLlama"
  wandb_run_name: "tinyllama-1.1b-slimpajama-v4-32"
  wandb_entity: "nathanngtruong-university-of-california-berkeley"

evaluation:
  enabled: true
  eval_interval: 5000
  tasks:
    - "hellaswag"
    - "arc_easy"
    - "arc_challenge"
    - "piqa"
    - "winogrande"
  num_fewshot: 0
  batch_size: 16
  limit: 1000
  log_individual_tasks: true
  log_aggregate_score: true
